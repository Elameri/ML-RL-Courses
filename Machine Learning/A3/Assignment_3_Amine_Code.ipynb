{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ge4MoUozAols"
   },
   "source": [
    "## INF8245E (Fall 2021) : Machine Learning - Assignment 3\n",
    "## Amine EL AMERI - Matricule: 2164634"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook has only been tested on jupyter locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 452,
     "status": "ok",
     "timestamp": 1632961878136,
     "user": {
      "displayName": "Amine EL AMERI",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11543486035736550386"
     },
     "user_tz": 240
    },
    "id": "wdPFWE-OB7Wi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv, pinv\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [25, 6]\n",
    "import pandas as pd\n",
    "from scipy.stats import mode\n",
    "import sys\n",
    "import copy\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmQndVCphhBo"
   },
   "source": [
    "## Medical Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['medical_dataset/test.csv',\n",
       " 'medical_dataset/train.csv',\n",
       " 'medical_dataset/valid.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myPath = \"medical_dataset/\"\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "files = [(myPath + f) for f in listdir(myPath) if isfile(join(myPath, f))]\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('medical_dataset/train.csv', header=None)\n",
    "train_raw = train_raw.values.tolist()[1:]\n",
    "\n",
    "valid_raw = pd.read_csv('medical_dataset/valid.csv', header=None)\n",
    "valid_raw = valid_raw.values.tolist()[1:]\n",
    "\n",
    "test_raw = pd.read_csv('medical_dataset/test.csv', header=None)\n",
    "test_raw = test_raw.values.tolist()[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### bag-of-words representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_pre_processing(data_raw, is_training_set):\n",
    "      \n",
    "    nbr_transcripts = len(data_raw)\n",
    "    data_processed = copy.deepcopy(data_raw)\n",
    "    all_words = {}\n",
    "    transcripts_words = []\n",
    "    \n",
    "    for i in range(nbr_transcripts):\n",
    "        \n",
    "        words = {}\n",
    "        \n",
    "        print(\"\\rTranscript number {}/{}.\".format(i+1, nbr_transcripts), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        transcript = data_raw[i][1]\n",
    "        last_space = -1\n",
    "        \n",
    "        for letter_id in range(len(transcript)):\n",
    "\n",
    "            if transcript[letter_id] in {\",\", \".\", \"'\", \":\", \";\", \"?\", \"!\", \"-\", \"+\", \"*\", \"/\", \"=\", \"[\", \"]\", \"(\", \")\", \"{\", \"}\", \" \", \"\\t\", \"\\n\"}:\n",
    "                word = transcript[last_space+1: letter_id].lower().strip()\n",
    "                last_space = letter_id\n",
    "                if word != \"\":\n",
    "                    if word in words.keys():\n",
    "                        words[word] += 1\n",
    "                    else:\n",
    "                        words[word] = 1\n",
    "                        \n",
    "                    if word in all_words.keys():\n",
    "                        all_words[word] += 1\n",
    "                    else:\n",
    "                        all_words[word] = 1\n",
    "\n",
    "        word = transcript[last_space+1:-1].lower().strip()\n",
    "        if word != \"\":\n",
    "            if word in words.keys():\n",
    "                words[word] += 1\n",
    "            else:\n",
    "                words[word] = 1\n",
    "            \n",
    "            if word in all_words.keys():\n",
    "                all_words[word] += 1\n",
    "            else:\n",
    "                all_words[word] = 1\n",
    "                \n",
    "        transcripts_words.append(words)\n",
    "        data_processed[i][1] = list(words.keys())\n",
    "\n",
    "    if is_training_set:\n",
    "        sorted_words = sorted(all_words.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        vocab_words = [el[0] for el in sorted_words][0:10000]\n",
    "        return (data_processed, vocab_words, sorted_words, transcripts_words) \n",
    "    else:\n",
    "        return data_processed, transcripts_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript number 4000/4000."
     ]
    }
   ],
   "source": [
    "# pre-process training data\n",
    "train_processed, vocab_words, train_words_dict, transcripts_words_train = bow_pre_processing(train_raw, is_training_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript number 499/499."
     ]
    }
   ],
   "source": [
    "# pre-process validation data\n",
    "valid_processed, transcripts_words_valid = bow_pre_processing(valid_raw, is_training_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript number 500/500."
     ]
    }
   ],
   "source": [
    "# pre-process testing data\n",
    "test_processed, transcripts_words_test = bow_pre_processing(test_raw, is_training_set=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### saving vocab data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save medical_text-vocab.txt\n",
    "data_tosave_vocab = [ (train_words_dict[idx][0] + \"\\t\" + str(idx+1) + \"\\t\" + str(train_words_dict[idx][1]) + \"\\n\") for idx in range(10000)]\n",
    "with open(\"medical_text-vocab.txt\", \"w\") as output:\n",
    "    for line in data_tosave_vocab:\n",
    "        output.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### binary bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbow(transcripts_words, vocab_words):\n",
    "    \n",
    "    transcripts_bbow = []\n",
    "    nbr_transcripts = len(transcripts_words)\n",
    "    \n",
    "    for transcript_id in range(nbr_transcripts):\n",
    "        \n",
    "        print(\"\\rTranscript number {}/{}.\".format(transcript_id+1, nbr_transcripts), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        dict_vocab = dict.fromkeys(vocab_words, 0)\n",
    "        for word in transcripts_words[transcript_id].keys():\n",
    "            if word in dict_vocab.keys():\n",
    "                dict_vocab[word] = 1\n",
    "        \n",
    "\n",
    "        transcripts_bbow.append(dict_vocab)\n",
    "\n",
    "    return transcripts_bbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript number 4000/4000."
     ]
    }
   ],
   "source": [
    "transcripts_bbow_train = bbow(transcripts_words_train, vocab_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript number 499/499."
     ]
    }
   ],
   "source": [
    "transcripts_bbow_valid = bbow(transcripts_words_valid, vocab_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript number 500/500."
     ]
    }
   ],
   "source": [
    "transcripts_bbow_test = bbow(transcripts_words_test, vocab_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### frequency bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fbow(transcripts_words, vocab_words):\n",
    "    \n",
    "    transcripts_fbow = []\n",
    "    nbr_transcripts = len(transcripts_words)\n",
    "    \n",
    "    for transcript_id in range(nbr_transcripts):\n",
    "        \n",
    "        print(\"\\rTranscript number {}/{}.\".format(transcript_id+1, nbr_transcripts), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        dict_vocab = dict.fromkeys(vocab_words, 0)\n",
    "        for word in transcripts_words[transcript_id].keys():\n",
    "            if word in dict_vocab.keys():\n",
    "                dict_vocab[word] += transcripts_words[transcript_id][word]\n",
    "        \n",
    "        sum_occ = sum(dict_vocab.values())\n",
    "        for key in dict_vocab.keys():\n",
    "            dict_vocab[key] = dict_vocab[key]/sum_occ\n",
    "        transcripts_fbow.append(dict_vocab)\n",
    "\n",
    "    return transcripts_fbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript number 4000/4000."
     ]
    }
   ],
   "source": [
    "transcripts_fbow_train = fbow(transcripts_words_train, vocab_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript number 499/499."
     ]
    }
   ],
   "source": [
    "transcripts_fbow_valid = fbow(transcripts_words_valid, vocab_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript number 500/500."
     ]
    }
   ],
   "source": [
    "transcripts_fbow_test = fbow(transcripts_words_test, vocab_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### dataset submission train, valid and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_subm(data_processed, vocab_words, filename):\n",
    "        \n",
    "    dict_vocab = dict.fromkeys(vocab_words, 0)\n",
    "    nbr_transcripts = len(data_processed)\n",
    "    \n",
    "    with open(filename, \"w\") as output:\n",
    "    \n",
    "        for transcript_id in range(nbr_transcripts):\n",
    "\n",
    "            print(\"\\rTranscript number {}/{}.\".format(transcript_id+1, nbr_transcripts), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            tmp = \"\"\n",
    "            for word_id in range(len(data_processed[transcript_id][1])):\n",
    "                if data_processed[transcript_id][1][word_id] in dict_vocab.keys():\n",
    "                    tmp += str(list(dict_vocab.keys()).index(data_processed[transcript_id][1][word_id])).strip() + \" \"\n",
    "                \n",
    "                if word_id == len(data_processed[transcript_id][1])-1:\n",
    "                    tmp = tmp.rstrip() + \"\\t\" + data_processed[transcript_id][0] + \"\\n\"\n",
    "\n",
    "            output.write(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript number 4000/4000."
     ]
    }
   ],
   "source": [
    "dataset_subm(train_processed, vocab_words, \"medical_text-train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript number 499/499."
     ]
    }
   ],
   "source": [
    "dataset_subm(valid_processed, vocab_words, \"medical_text-valid.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript number 500/500."
     ]
    }
   ],
   "source": [
    "dataset_subm(test_processed, vocab_words, \"medical_text-test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Q2-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_classifier(data_processed):\n",
    "    predictions = []\n",
    "    for i in range(len(data_processed)):\n",
    "        lbl = random.randint(1,4)\n",
    "        predictions.append(lbl)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_class_classifier(train_processed, data_processed):\n",
    "\n",
    "    majority_class = np.argmax([[train_processed[i][0] for i in range(len(train_processed))].count(j) for j in [\"1\", \"2\", \"3\", \"4\"]])+1\n",
    "    predictions = [majority_class for i in range(len(data_processed))]\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the F1 score of the random classifier is 0.2610138420043496\n"
     ]
    }
   ],
   "source": [
    "# F1 score of the random classifier\n",
    "\n",
    "y_pred = random_classifier(train_processed)\n",
    "y_true = [int(train_processed[i][0]) for i in range(len(train_processed))]\n",
    "\n",
    "f1_score_random_classifier = f1_score(y_true, y_pred, average='macro')\n",
    "print(f\"the F1 score of the random classifier is {f1_score_random_classifier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the F1 score of the majority classifier is 0.120996778472617\n"
     ]
    }
   ],
   "source": [
    "# F1 score of the majority class classifier\n",
    "\n",
    "y_pred = majority_class_classifier(train_processed, train_processed)\n",
    "y_true = [int(train_processed[i][0]) for i in range(len(train_processed))]\n",
    "\n",
    "f1_score_majority_class_classifier = f1_score(y_true, y_pred, average='macro')\n",
    "print(f\"the F1 score of the majority classifier is {f1_score_majority_class_classifier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Q2-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbow_train_good_format = np.array([list(transcripts_bbow_train[i].values()) for i in range(len(transcripts_bbow_train))])\n",
    "bbow_valid_good_format = np.array([list(transcripts_bbow_valid[i].values()) for i in range(len(transcripts_bbow_valid))])\n",
    "bbow_test_good_format = np.array([list(transcripts_bbow_test[i].values()) for i in range(len(transcripts_bbow_test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_good_format = [int(train_processed[i][0]) for i in range(len(train_processed))]\n",
    "y_valid_good_format = [int(valid_processed[i][0]) for i in range(len(valid_processed))]\n",
    "y_test_good_format = [int(test_processed[i][0]) for i in range(len(test_processed))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAndValid = np.concatenate((bbow_train_good_format, bbow_valid_good_format), axis=0)\n",
    "y_trainAndValid = np.concatenate((y_train_good_format, y_valid_good_format))\n",
    "split_index = np.concatenate((-np.ones(bbow_train_good_format.shape[0]), np.zeros(bbow_valid_good_format.shape[0])), axis=0)\n",
    "predef_split = PredefinedSplit(test_fold = split_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:508: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best BernoulliNB classifier is BernoulliNB(alpha=0.65)\n"
     ]
    }
   ],
   "source": [
    "smooth_params = {'alpha': np.arange(0, 1.05, 0.05)}\n",
    "\n",
    "bnb_clf = GridSearchCV(estimator = BernoulliNB(), \n",
    "                       param_grid = smooth_params, \n",
    "                       scoring = 'f1_macro', \n",
    "                       cv = predef_split)\n",
    "\n",
    "bnb_clf.fit(trainAndValid, y_trainAndValid)\n",
    "print(f\"The best BernoulliNB classifier is {bnb_clf.best_estimator_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB train f1_score with best param: 0.537771413178238\n",
      "BernoulliNB valid f1_score with best param: 0.46002458735798557\n",
      "BernoulliNB test f1_score with best param: 0.4703643259118477\n"
     ]
    }
   ],
   "source": [
    "bnb = BernoulliNB(alpha = 0.65, binarize = None, fit_prior = True)\n",
    "\n",
    "bnb.fit(bbow_train_good_format, y_train_good_format)\n",
    "y_pred_train = bnb.predict(bbow_train_good_format)\n",
    "y_pred_valid = bnb.predict(bbow_valid_good_format)\n",
    "y_pred_test = bnb.predict(bbow_test_good_format)\n",
    "\n",
    "print(f\"BernoulliNB train f1_score with best param: {f1_score(y_train_good_format, y_pred_train, average='macro')}\")\n",
    "print(f\"BernoulliNB valid f1_score with best param: {f1_score(y_valid_good_format, y_pred_valid, average='macro')}\")\n",
    "print(f\"BernoulliNB test f1_score with best param: {f1_score(y_test_good_format, y_pred_test, average='macro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best decision tree classifier parameters are: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 0.1}\n"
     ]
    }
   ],
   "source": [
    "params_to_test = {'criterion': [\"gini\", \"entropy\"], 'max_depth': np.arange(10, 80, 20), 'min_samples_split': np.arange(0.1, 1, 0.2)}\n",
    "\n",
    "tree_clf = GridSearchCV(estimator = DecisionTreeClassifier(), \n",
    "                        param_grid = params_to_test, \n",
    "                        scoring = 'f1_macro', \n",
    "                        cv = predef_split)\n",
    "\n",
    "tree_clf.fit(trainAndValid, y_trainAndValid)\n",
    "print(f\"the best decision tree classifier parameters are: {tree_clf.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier train f1_score with best param: 0.7313573608654307\n",
      "DecisionTreeClassifier valid f1_score with best param: 0.7097091502926711\n",
      "DecisionTreeClassifier test f1_score with best param: 0.7118875760543775\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(criterion='gini', max_depth=30, min_samples_split=0.1)\n",
    "tree.fit(bbow_train_good_format, y_train_good_format)\n",
    "\n",
    "y_pred_train = tree.predict(bbow_train_good_format)\n",
    "y_pred_valid = tree.predict(bbow_valid_good_format)\n",
    "y_pred_test = tree.predict(bbow_test_good_format)\n",
    "\n",
    "print(f\"DecisionTreeClassifier train f1_score with best param: {f1_score(y_train_good_format, y_pred_train, average='macro')}\")\n",
    "print(f\"DecisionTreeClassifier valid f1_score with best param: {f1_score(y_valid_good_format, y_pred_valid, average='macro')}\")\n",
    "print(f\"DecisionTreeClassifier test f1_score with best param: {f1_score(y_test_good_format, y_pred_test, average='macro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best logistic regression classifier parameters are: {'C': 0.1, 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "params_to_test = {'penalty': ['l1','l2'], 'C': [0.01, 0.05, 0.1, 1, 5, 10, 50]}\n",
    "\n",
    "log_reg_clf = GridSearchCV(estimator = LogisticRegression(solver='liblinear'), \n",
    "                           param_grid = params_to_test, \n",
    "                           scoring = 'f1_macro', \n",
    "                           cv = predef_split)\n",
    "\n",
    "log_reg_clf.fit(trainAndValid, y_trainAndValid)\n",
    "print(f\"the best logistic regression classifier parameters are: {log_reg_clf.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression train f1_score with best param: 0.8238065644344815\n",
      "LogisticRegression valid f1_score with best param: 0.7993613135811319\n",
      "LogisticRegression test f1_score with best param: 0.8172132253711201\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(solver='liblinear', penalty='l1', C=0.1)\n",
    "log_reg.fit(bbow_train_good_format, y_train_good_format)\n",
    "\n",
    "y_pred_train = log_reg.predict(bbow_train_good_format)\n",
    "y_pred_valid = log_reg.predict(bbow_valid_good_format)\n",
    "y_pred_test = log_reg.predict(bbow_test_good_format)\n",
    "\n",
    "print(f\"LogisticRegression train f1_score with best param: {f1_score(y_train_good_format, y_pred_train, average='macro')}\")\n",
    "print(f\"LogisticRegression valid f1_score with best param: {f1_score(y_valid_good_format, y_pred_valid, average='macro')}\")\n",
    "print(f\"LogisticRegression test f1_score with best param: {f1_score(y_test_good_format, y_pred_test, average='macro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best linear SVM classifier parameters are: {'C': 0.093}\n"
     ]
    }
   ],
   "source": [
    "params_to_test = {'C': np.arange(0.001, 0.1, 0.001)}\n",
    "\n",
    "linear_SVM_clf = GridSearchCV(estimator = LinearSVC(random_state=0, max_iter=6000), \n",
    "                              param_grid = params_to_test, \n",
    "                              scoring = 'f1_macro', \n",
    "                              cv = predef_split)\n",
    "\n",
    "linear_SVM_clf.fit(trainAndValid, y_trainAndValid)\n",
    "print(f\"the best linear SVM classifier parameters are: {linear_SVM_clf.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC train f1_score with best param: 0.9075425680813174\n",
      "LinearSVC valid f1_score with best param: 0.7383026509927384\n",
      "LinearSVC test f1_score with best param: 0.7837180724102886\n"
     ]
    }
   ],
   "source": [
    "linearSVM = LinearSVC(random_state=0, max_iter=6000, C = 0.093)\n",
    "linearSVM.fit(bbow_train_good_format, y_train_good_format)\n",
    "\n",
    "y_pred_train = linearSVM.predict(bbow_train_good_format)\n",
    "y_pred_valid = linearSVM.predict(bbow_valid_good_format)\n",
    "y_pred_test = linearSVM.predict(bbow_test_good_format)\n",
    "\n",
    "print(f\"LinearSVC train f1_score with best param: {f1_score(y_train_good_format, y_pred_train, average='macro')}\")\n",
    "print(f\"LinearSVC valid f1_score with best param: {f1_score(y_valid_good_format, y_pred_valid, average='macro')}\")\n",
    "print(f\"LinearSVC test f1_score with best param: {f1_score(y_test_good_format, y_pred_test, average='macro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbow_train_good_format = np.array([list(transcripts_fbow_train[i].values()) for i in range(len(transcripts_fbow_train))])\n",
    "fbow_valid_good_format = np.array([list(transcripts_fbow_valid[i].values()) for i in range(len(transcripts_fbow_valid))])\n",
    "fbow_test_good_format = np.array([list(transcripts_fbow_test[i].values()) for i in range(len(transcripts_fbow_test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAndValid = np.concatenate((fbow_train_good_format, fbow_valid_good_format), axis=0)\n",
    "y_trainAndValid = np.concatenate((y_train_good_format, y_valid_good_format))\n",
    "split_index = np.concatenate((-np.ones(fbow_train_good_format.shape[0]), np.zeros(fbow_valid_good_format.shape[0])), axis=0)\n",
    "predef_split = PredefinedSplit(test_fold = split_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB train f1_score: 0.6904713749154733\n",
      "GaussianNB valid f1_score: 0.3638831094505836\n",
      "GaussianNB test f1_score: 0.352960329522165\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "\n",
    "gnb.fit(fbow_train_good_format, y_train_good_format)\n",
    "y_pred_train = gnb.predict(fbow_train_good_format)\n",
    "y_pred_valid = gnb.predict(fbow_valid_good_format)\n",
    "y_pred_test = gnb.predict(fbow_test_good_format)\n",
    "\n",
    "print(f\"GaussianNB train f1_score: {f1_score(y_train_good_format, y_pred_train, average='macro')}\")\n",
    "print(f\"GaussianNB valid f1_score: {f1_score(y_valid_good_format, y_pred_valid, average='macro')}\")\n",
    "print(f\"GaussianNB test f1_score: {f1_score(y_test_good_format, y_pred_test, average='macro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best decision tree classifier parameters are: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 0.1}\n"
     ]
    }
   ],
   "source": [
    "params_to_test = {'criterion': [\"gini\", \"entropy\"], 'max_depth': np.arange(10, 80, 20), 'min_samples_split': np.arange(0.1, 1, 0.2)}\n",
    "\n",
    "tree_clf = GridSearchCV(estimator = DecisionTreeClassifier(), \n",
    "                        param_grid = params_to_test, \n",
    "                        scoring = 'f1_macro', \n",
    "                        cv = predef_split)\n",
    "\n",
    "tree_clf.fit(trainAndValid, y_trainAndValid)\n",
    "print(f\"the best decision tree classifier parameters are: {tree_clf.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier train f1_score with best param: 0.7328245647524696\n",
      "DecisionTreeClassifier valid f1_score with best param: 0.7216274221878163\n",
      "DecisionTreeClassifier test f1_score with best param: 0.7090111767169525\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(criterion='gini', max_depth=30, min_samples_split=0.1)\n",
    "tree.fit(fbow_train_good_format, y_train_good_format)\n",
    "\n",
    "y_pred_train = tree.predict(fbow_train_good_format)\n",
    "y_pred_valid = tree.predict(fbow_valid_good_format)\n",
    "y_pred_test = tree.predict(fbow_test_good_format)\n",
    "\n",
    "print(f\"DecisionTreeClassifier train f1_score with best param: {f1_score(y_train_good_format, y_pred_train, average='macro')}\")\n",
    "print(f\"DecisionTreeClassifier valid f1_score with best param: {f1_score(y_valid_good_format, y_pred_valid, average='macro')}\")\n",
    "print(f\"DecisionTreeClassifier test f1_score with best param: {f1_score(y_test_good_format, y_pred_test, average='macro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best logistic regression classifier parameters are: {'C': 50, 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "params_to_test = {'penalty': ['l1','l2'], 'C': [0.01, 0.05, 0.1, 1, 5, 10, 50]}\n",
    "\n",
    "log_reg_clf = GridSearchCV(estimator = LogisticRegression(solver='liblinear'), \n",
    "                           param_grid = params_to_test, \n",
    "                           scoring = 'f1_macro', \n",
    "                           cv = predef_split)\n",
    "\n",
    "log_reg_clf.fit(trainAndValid, y_trainAndValid)\n",
    "print(f\"the best logistic regression classifier parameters are: {log_reg_clf.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression train f1_score with best param: 0.8388700393491383\n",
      "LogisticRegression valid f1_score with best param: 0.760287094912295\n",
      "LogisticRegression test f1_score with best param: 0.7653840822497652\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(solver='liblinear', penalty='l1', C=50)\n",
    "log_reg.fit(fbow_train_good_format, y_train_good_format)\n",
    "\n",
    "y_pred_train = log_reg.predict(fbow_train_good_format)\n",
    "y_pred_valid = log_reg.predict(fbow_valid_good_format)\n",
    "y_pred_test = log_reg.predict(fbow_test_good_format)\n",
    "\n",
    "print(f\"LogisticRegression train f1_score with best param: {f1_score(y_train_good_format, y_pred_train, average='macro')}\")\n",
    "print(f\"LogisticRegression valid f1_score with best param: {f1_score(y_valid_good_format, y_pred_valid, average='macro')}\")\n",
    "print(f\"LogisticRegression test f1_score with best param: {f1_score(y_test_good_format, y_pred_test, average='macro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best linear SVM classifier parameters are: {'C': 0.089}\n"
     ]
    }
   ],
   "source": [
    "params_to_test = {'C': np.arange(0.001, 0.1, 0.001)}\n",
    "\n",
    "linear_SVM_clf = GridSearchCV(estimator = LinearSVC(random_state=0, max_iter=6000), \n",
    "                              param_grid = params_to_test, \n",
    "                              scoring = 'f1_macro', \n",
    "                              cv = predef_split)\n",
    "\n",
    "linear_SVM_clf.fit(trainAndValid, y_trainAndValid)\n",
    "print(f\"the best linear SVM classifier parameters are: {linear_SVM_clf.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC train f1_score with best param: 0.31937742267309815\n",
      "LinearSVC valid f1_score with best param: 0.3179102706124387\n",
      "LinearSVC test f1_score with best param: 0.3129974362578737\n"
     ]
    }
   ],
   "source": [
    "linearSVM = LinearSVC(random_state=0, max_iter=6000, C = 0.089)\n",
    "linearSVM.fit(fbow_train_good_format, y_train_good_format)\n",
    "\n",
    "y_pred_train = linearSVM.predict(fbow_train_good_format)\n",
    "y_pred_valid = linearSVM.predict(fbow_valid_good_format)\n",
    "y_pred_test = linearSVM.predict(fbow_test_good_format)\n",
    "\n",
    "print(f\"LinearSVC train f1_score with best param: {f1_score(y_train_good_format, y_pred_train, average='macro')}\")\n",
    "print(f\"LinearSVC valid f1_score with best param: {f1_score(y_valid_good_format, y_pred_valid, average='macro')}\")\n",
    "print(f\"LinearSVC test f1_score with best param: {f1_score(y_test_good_format, y_pred_test, average='macro')}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_1_Amine_Code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
